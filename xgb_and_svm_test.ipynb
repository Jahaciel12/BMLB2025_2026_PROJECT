{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a30d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3e50e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare models\n",
      "\n",
      "Evaluating XGBoost\n",
      "XGBoost - F1: 0.8096, ROC AUC: 0.9515\n",
      "Evaluating SVM\n",
      "SVM - F1: 0.8728, ROC AUC: 0.9765\n",
      "\n",
      "Final cross-validation results:\n",
      "XGBoost    - F1: 0.8096, ROC AUC: 0.9515 \n",
      "SVM        - F1: 0.8728, ROC AUC: 0.9765 \n"
     ]
    }
   ],
   "source": [
    "# XGB and SVM pipeline and training\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('training_balanced_data.csv')\n",
    "X = data.drop(columns=['Outcome', 'Id'])\n",
    "y = data['Outcome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, X_test, y_test): # evaluates trained model on test data\n",
    "    y_pred = model.predict(X_test) \n",
    "    y_pred_proba = model.predict_proba(X_test) \n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
    "    \n",
    "    return f1, roc_auc # returns macro f1 score and roc_auc score\n",
    "\n",
    "# Pipelines functions\n",
    "def create_xgb_pipeline(): # pipeline function for xgb\n",
    "    return Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=3)), # fill missing values using KNNImputer\n",
    "        ('scaler', StandardScaler()), # feature scaling\n",
    "        ('xgb', XGBClassifier(random_state=42, eval_metric='mlogloss')) # xgb classifier\n",
    "    ])\n",
    "\n",
    "def create_svm_pipeline(): # pipeline function for svm\n",
    "    return Pipeline([ \n",
    "        ('imputer', KNNImputer(n_neighbors=3)), # fix missing values using KNNImputer\n",
    "        ('scaler', StandardScaler()), # feature scaling\n",
    "        ('pca', PCA(n_components=100)), # reduces the dimensionality to 100\n",
    "        ('svm', SVC(probability=True, random_state=42)) # svm classifier with probability\n",
    "    ])\n",
    "\n",
    "# Cross-validation\n",
    "def evaluate_all_models(X_train, y_train):\n",
    "\n",
    "    models = { \n",
    "        'XGBoost': create_xgb_pipeline(), # create xgb pipeline\n",
    "        'SVM': create_svm_pipeline() # create svm pipline\n",
    "    }\n",
    "\n",
    "# cross validation with 10 folds\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) \n",
    "    \n",
    "    results = {} # store metrics for each model\n",
    "\n",
    "# Loop evaluation for each model\n",
    "    for name, model in models.items(): \n",
    "        print(f\"Evaluating {name}\") # print which model is currently evaluated\n",
    "        \n",
    "        try: # handy code for catching any errors during the process\n",
    "            cv_scores_f1 = cross_val_score(model, X_train, y_train, cv=cv, # perform 10 fold cross validation for f1 score\n",
    "                                           scoring='f1_macro', n_jobs=-2)\n",
    "            cv_scores_roc = cross_val_score(model, X_train, y_train, cv=cv, # perform 10 fold cros validation for ROC AUC score\n",
    "                                            scoring='roc_auc_ovr', n_jobs=-2)\n",
    "\n",
    "            results[name] = { # store the mean of metrics in results\n",
    "                'f1_mean': cv_scores_f1.mean(), \n",
    "                'roc_auc_mean': cv_scores_roc.mean(),\n",
    "                \n",
    "            }\n",
    "            # print metric scores\n",
    "            print(f\"{name} - F1: {cv_scores_f1.mean():.4f}, \"\n",
    "                  f\"ROC AUC: {cv_scores_roc.mean():.4f}\")\n",
    "\n",
    "        except Exception as e: # handy code for handeling errors. it continues with the next model\n",
    "            print(f\"Error evaluating {name}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# model comparison \n",
    "print(\"Compare models\\n\")\n",
    "results = evaluate_all_models(X_train, y_train) # gives the scores of each model\n",
    "\n",
    "print(\"hello\")\n",
    "# Print summary of results\n",
    "print(\"\\nFinal cross-validation results:\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name:10} - F1: {metrics['f1_mean']:.4f}, \"\n",
    "          f\"ROC AUC: {metrics['roc_auc_mean']:.4f} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0fae3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pca__n_components': 150, 'svm__C': 2, 'svm__gamma': 'scale'}\n",
      "0.873167734046109\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparamter tuning for SVM\n",
    "\n",
    "# SVM tuning pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', KNNImputer(n_neighbors=3)), # fix missing values using KNNImputer\n",
    "    ('scaler', StandardScaler()), # feature scaling\n",
    "    ('pca', PCA()), # PCA, but now this will be scaled\n",
    "    ('svm', SVC(probability=True, random_state=42)) # svm classifier with probabilities\n",
    "])\n",
    "\n",
    "# define hyperparameter grid \n",
    "param_grid = {\n",
    "    'pca__n_components': [50, 100, 150], # number of PCA components to try\n",
    "    'svm__C': [0.5, 1, 2, 5], # Controls the trade-off between a wider margin (low C) and correctly classifying all points (high C)\n",
    "    'svm__gamma': ['scale', 0.01, 0.001] # Determines how far the influence of each data point reaches with high gamma fitting tightly to the data\n",
    "}\n",
    "\n",
    "# GridsearchCV setup\n",
    "grid = GridSearchCV(\n",
    "    pipeline, # pipeline to tune\n",
    "    param_grid, # defined parameter combinations\n",
    "    cv=10, # 10 fold Cross validation\n",
    "    scoring='f1_macro', # test metric\n",
    "    n_jobs=-2, # use all but one CPU cores\n",
    ")\n",
    "\n",
    "# train model for all parameter combinations\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# print best parameter \n",
    "print(grid.best_params_)\n",
    "\n",
    "# print best cross validation f1 score\n",
    "print(grid.best_score_)\n",
    "\n",
    "# save best model\n",
    "best_svm = grid.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b886b36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final prediciton on test set:\n",
      "F1 Score: 0.8942\n",
      "ROC AUC: 0.9818\n"
     ]
    }
   ],
   "source": [
    "# Train the best model on full training data\n",
    "\n",
    "# Load data \n",
    "data = pd.read_csv('training_balanced_data.csv')\n",
    "X = data.drop(columns=['Outcome', 'Id'])\n",
    "y = data['Outcome']\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# train model with the best gridsearch parameter\n",
    "best_svm.fit(X_train, y_train)\n",
    "\n",
    "# Prediction of final model\n",
    "y_pred = best_svm.predict(X_test)\n",
    "y_pred_proba = best_svm.predict_proba(X_test)\n",
    "\n",
    "# compute matrics\n",
    "f1 = f1_score(y_test, y_pred, average='macro') # macro f1 score\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro') # ROC AUC ovr  score\n",
    "\n",
    "print(\"\\nFinal prediciton on test set:\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
