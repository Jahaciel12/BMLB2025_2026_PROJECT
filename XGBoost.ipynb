{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb231112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional dependencies\n",
    "# Import dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "import keras\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load and prepare the data (assuming this part is already done)\n",
    "# X_train, X_test, y_train, y_test should be available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8881128f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating all models...\n",
      "Evaluating XGBoost...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/anaconda3/envs/bmlproj/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/envs/bmlproj/lib/python3.11/site-packages/sklearn/base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/bmlproj/lib/python3.11/site-packages/imblearn/pipeline.py\", line 518, in fit\n    Xt, yt = self._fit(X, y, routed_params, raw_params=params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/bmlproj/lib/python3.11/site-packages/imblearn/pipeline.py\", line 400, in _fit\n    self._validate_steps()\n  File \"/opt/anaconda3/envs/bmlproj/lib/python3.11/site-packages/imblearn/pipeline.py\", line 289, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps of the chain should not be Pipelines\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 160\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaluating all models...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m results, models = \u001b[43mevaluate_all_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# Step 8: Hyperparameter optimization for the best model\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOptimizing hyperparameters for XGBoost...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 141\u001b[39m, in \u001b[36mevaluate_all_models\u001b[39m\u001b[34m(X_train, y_train)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# Cross-validation scores\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m cv_scores_f1 = \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mf1_macro\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m cv_scores_roc = cross_val_score(model, X_train, y_train,\n\u001b[32m    144\u001b[39m                               cv=cv, scoring=\u001b[33m'\u001b[39m\u001b[33mroc_auc_ovr\u001b[39m\u001b[33m'\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m    146\u001b[39m results[name] = {\n\u001b[32m    147\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mf1_mean\u001b[39m\u001b[33m'\u001b[39m: cv_scores_f1.mean(),\n\u001b[32m    148\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mf1_std\u001b[39m\u001b[33m'\u001b[39m: cv_scores_f1.std(),\n\u001b[32m    149\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mroc_auc_mean\u001b[39m\u001b[33m'\u001b[39m: cv_scores_roc.mean(),\n\u001b[32m    150\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mroc_auc_std\u001b[39m\u001b[33m'\u001b[39m: cv_scores_roc.std()\n\u001b[32m    151\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bmlproj/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bmlproj/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:677\u001b[39m, in \u001b[36mcross_val_score\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[32m    675\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m cv_results = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[33m\"\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bmlproj/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bmlproj/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:419\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    398\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m    399\u001b[39m results = parallel(\n\u001b[32m    400\u001b[39m     delayed(_fit_and_score)(\n\u001b[32m    401\u001b[39m         clone(estimator),\n\u001b[32m   (...)\u001b[39m\u001b[32m    416\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[32m    417\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bmlproj/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:505\u001b[39m, in \u001b[36m_warn_or_raise_about_fit_failures\u001b[39m\u001b[34m(results, error_score)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits == num_fits:\n\u001b[32m    499\u001b[39m     all_fits_failed_message = (\n\u001b[32m    500\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    501\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can try to debug the error by setting error_score=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    503\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    508\u001b[39m     some_fits_failed_message = (\n\u001b[32m    509\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe score on these train-test partitions for these parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    514\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: \nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/anaconda3/envs/bmlproj/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/envs/bmlproj/lib/python3.11/site-packages/sklearn/base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/bmlproj/lib/python3.11/site-packages/imblearn/pipeline.py\", line 518, in fit\n    Xt, yt = self._fit(X, y, routed_params, raw_params=params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/bmlproj/lib/python3.11/site-packages/imblearn/pipeline.py\", line 400, in _fit\n    self._validate_steps()\n  File \"/opt/anaconda3/envs/bmlproj/lib/python3.11/site-packages/imblearn/pipeline.py\", line 289, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps of the chain should not be Pipelines\n"
     ]
    }
   ],
   "source": [
    "# load balanced data\n",
    "data = pd.read_csv('training_balanced_data.csv')\n",
    "X = data.drop(columns=['Outcome', 'Id'])\n",
    "y = data['Outcome']\n",
    "\n",
    "#split in training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Step 1: Define evaluation function\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
    "    \n",
    "    return f1, roc_auc\n",
    "\n",
    "# Step 2: Enhanced preprocessing pipeline\n",
    "def create_enhanced_preprocessing():\n",
    "    preprocessing = Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=3)),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    return preprocessing\n",
    "\n",
    "# Step 3: Try different classifiers with optimized pipelines\n",
    "def create_xgb_pipeline():\n",
    "    preprocessing = create_enhanced_preprocessing()\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('xgb', XGBClassifier(random_state=42, eval_metric='mlogloss'))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def create_svm_pipeline():\n",
    "    preprocessing = create_enhanced_preprocessing()\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('pca', PCA(n_components=100)),\n",
    "        ('svm', SVC(probability=True, random_state=42))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def create_voting_pipeline():\n",
    "    preprocessing = create_enhanced_preprocessing()\n",
    "    \n",
    "    # Individual classifiers\n",
    "    rf = RandomForestClassifier(n_estimators=250, random_state=42)\n",
    "    xgb = XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "    lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    \n",
    "    # Voting classifier\n",
    "    voting = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', rf),\n",
    "            ('xgb', xgb),\n",
    "            ('lr', lr)\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('voting', voting)\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def create_balanced_rf_pipeline():\n",
    "    preprocessing = create_enhanced_preprocessing()\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('balanced_rf', BalancedRandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# Step 4: Hyperparameter tuning for the best performing model\n",
    "def optimize_xgb_hyperparameters(X_train, y_train):\n",
    "    preprocessing = create_enhanced_preprocessing()\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('xgb', XGBClassifier(random_state=42, eval_metric='mlogloss'))\n",
    "    ])\n",
    "    \n",
    "    param_grid = {\n",
    "        'xgb__n_estimators': [100, 200, 300],\n",
    "        'xgb__max_depth': [3, 5, 7],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'xgb__subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grid, cv=10, scoring='f1_macro', \n",
    "        n_jobs=-2, verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_score_\n",
    "\n",
    "# Step 5: Try SMOTE for handling class imbalance\n",
    "def create_smote_pipeline():\n",
    "    preprocessing = create_enhanced_preprocessing()\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('xgb', XGBClassifier(random_state=42, eval_metric='mlogloss'))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# Step 6: Feature selection approach\n",
    "def create_feature_selection_pipeline():\n",
    "    preprocessing = create_enhanced_preprocessing()\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('feature_selection', SelectKBest(f_classif, k=500)),\n",
    "        ('xgb', XGBClassifier(random_state=42, eval_metric='mlogloss'))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# Step 7: Evaluate all models using cross-validation\n",
    "def evaluate_all_models(X_train, y_train):\n",
    "    models = {\n",
    "        'XGBoost': create_xgb_pipeline(),\n",
    "        'SVM': create_svm_pipeline(),\n",
    "        'Voting_Classifier': create_voting_pipeline(),\n",
    "        'Balanced_RF': create_balanced_rf_pipeline(),\n",
    "        'SMOTE_XGB': create_smote_pipeline(),\n",
    "        'Feature_Selection_XGB': create_feature_selection_pipeline()\n",
    "    }\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        \n",
    "        # Cross-validation scores\n",
    "        cv_scores_f1 = cross_val_score(model, X_train, y_train, \n",
    "                                     cv=cv, scoring='f1_macro', n_jobs=-1)\n",
    "        cv_scores_roc = cross_val_score(model, X_train, y_train,\n",
    "                                      cv=cv, scoring='roc_auc_ovr', n_jobs=-1)\n",
    "        \n",
    "        results[name] = {\n",
    "            'f1_mean': cv_scores_f1.mean(),\n",
    "            'f1_std': cv_scores_f1.std(),\n",
    "            'roc_auc_mean': cv_scores_roc.mean(),\n",
    "            'roc_auc_std': cv_scores_roc.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} - F1: {cv_scores_f1.mean():.4f} (±{cv_scores_f1.std():.4f}), \"\n",
    "              f\"ROC AUC: {cv_scores_roc.mean():.4f} (±{cv_scores_roc.std():.4f})\")\n",
    "    \n",
    "    return results, models\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Evaluating all models...\")\n",
    "results, models = evaluate_all_models(X_train, y_train)\n",
    "\n",
    "# Step 8: Hyperparameter optimization for the best model\n",
    "print(\"\\nOptimizing hyperparameters for XGBoost...\")\n",
    "best_xgb_model, best_score = optimize_xgb_hyperparameters(X_train, y_train)\n",
    "print(f\"Best cross-validation score: {best_score:.4f}\")\n",
    "\n",
    "# Add optimized model to our models dictionary\n",
    "models['Optimized_XGB'] = best_xgb_model\n",
    "\n",
    "# Step 9: Train final models on full training data and evaluate on test set\n",
    "print(\"\\nTraining final models on full training data...\")\n",
    "final_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    f1, roc_auc = evaluate_model(model, X_test, y_test)\n",
    "    final_results[name] = {'f1': f1, 'roc_auc': roc_auc}\n",
    "    print(f\"{name} - Test F1: {f1:.4f}, Test ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Step 10: Compare with previous models from steps 3, 5, 7, 9\n",
    "# (Assuming we have these models saved as: model_step3, model_step5, model_step7, model_step9)\n",
    "\n",
    "previous_models = {\n",
    "    'Step3_Model': make_pipeline(SimpleImputer(), StandardScaler(), LogisticRegression(max_iter=1000, penalty = None)),  # Replace with actual model from step 3\n",
    "    'Step5_Model': make_pipeline(StandardScaler(), KNNImputer(n_neighbors=3), LogisticRegression(C = 1)),  # Replace with actual model from step 5  \n",
    "    'Step7_Model': make_pipeline(StandardScaler(), KNNImputer(n_neighbors=3), PCA(n_components=100), LogisticRegression(C = 1)),  # Replace with actual model from step 7\n",
    "    'Step9_Model': make_pipeline(StandardScaler(), KNNImputer(n_neighbors=3), PCA(), LogisticRegression())   # Replace with actual model from step 9\n",
    "}\n",
    "\n",
    "print(\"\\nEvaluating previous step models on test set...\")\n",
    "for name, model in previous_models.items():\n",
    "    f1, roc_auc = evaluate_model(model, X_test, y_test)\n",
    "    final_results[name] = {'f1': f1, 'roc_auc': roc_auc}\n",
    "    print(f\"{name} - Test F1: {f1:.4f}, Test ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Step 11: Display final comparison\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL COMPARISON ON TEST SET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sort by F1 score\n",
    "sorted_results = sorted(final_results.items(), key=lambda x: x[1]['f1'], reverse=True)\n",
    "\n",
    "for name, scores in sorted_results:\n",
    "    print(f\"{name:20} - F1: {scores['f1']:.4f}, ROC AUC: {scores['roc_auc']:.4f}\")\n",
    "\n",
    "# Identify best model\n",
    "best_model_name, best_scores = sorted_results[0]\n",
    "print(f\"\\nBEST MODEL: {best_model_name}\")\n",
    "print(f\"F1 Score: {best_scores['f1']:.4f}\")\n",
    "print(f\"ROC AUC OvR: {best_scores['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a0994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional dependencies\n",
    "# Import dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "import keras\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load and prepare the data (assuming this part is already done)\n",
    "# X_train, X_test, y_train, y_test should be available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a59d8db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating all models...\n",
      "Evaluating XGBoost...\n",
      "XGBoost - F1: 0.7857 (±0.0126), ROC AUC: 0.9481 (±0.0088)\n",
      "Evaluating SVM...\n",
      "SVM - F1: 0.8571 (±0.0195), ROC AUC: 0.9736 (±0.0095)\n",
      "Evaluating Voting_Classifier...\n",
      "Voting_Classifier - F1: 0.8217 (±0.0214), ROC AUC: 0.9596 (±0.0085)\n",
      "Evaluating Balanced_RF...\n",
      "Balanced_RF - F1: 0.6807 (±0.0180), ROC AUC: 0.8704 (±0.0081)\n",
      "Evaluating SMOTE_XGB...\n",
      "SMOTE_XGB - F1: 0.7856 (±0.0266), ROC AUC: 0.9471 (±0.0101)\n",
      "Evaluating Feature_Selection_XGB...\n",
      "Feature_Selection_XGB - F1: 0.8070 (±0.0246), ROC AUC: 0.9552 (±0.0087)\n",
      "\n",
      "Optimizing hyperparameters for XGBoost...\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best cross-validation score: 0.8314\n",
      "\n",
      "Training final models on full training data...\n",
      "Training XGBoost...\n",
      "XGBoost - Test F1: 0.8275, Test ROC AUC: 0.9658\n",
      "Training SVM...\n",
      "SVM - Test F1: 0.8759, Test ROC AUC: 0.9773\n",
      "Training Voting_Classifier...\n",
      "Voting_Classifier - Test F1: 0.8753, Test ROC AUC: 0.9721\n",
      "Training Balanced_RF...\n",
      "Balanced_RF - Test F1: 0.6903, Test ROC AUC: 0.8910\n",
      "Training SMOTE_XGB...\n",
      "SMOTE_XGB - Test F1: 0.8272, Test ROC AUC: 0.9684\n",
      "Training Feature_Selection_XGB...\n",
      "Feature_Selection_XGB - Test F1: 0.8399, Test ROC AUC: 0.9658\n",
      "Training Optimized_XGB...\n",
      "Optimized_XGB - Test F1: 0.8773, Test ROC AUC: 0.9743\n",
      "\n",
      "Evaluating baseline models on test set...\n",
      "Baseline_LR - Test F1: 0.8043, Test ROC AUC: 0.9436\n",
      "Baseline_RF - Test F1: 0.7105, Test ROC AUC: 0.8897\n",
      "Baseline_XGB - Test F1: 0.8124, Test ROC AUC: 0.9614\n",
      "\n",
      "==================================================\n",
      "FINAL COMPARISON ON TEST SET\n",
      "==================================================\n",
      "Optimized_XGB             - F1: 0.8773, ROC AUC: 0.9743\n",
      "SVM                       - F1: 0.8759, ROC AUC: 0.9773\n",
      "Voting_Classifier         - F1: 0.8753, ROC AUC: 0.9721\n",
      "Feature_Selection_XGB     - F1: 0.8399, ROC AUC: 0.9658\n",
      "XGBoost                   - F1: 0.8275, ROC AUC: 0.9658\n",
      "SMOTE_XGB                 - F1: 0.8272, ROC AUC: 0.9684\n",
      "Baseline_XGB              - F1: 0.8124, ROC AUC: 0.9614\n",
      "Baseline_LR               - F1: 0.8043, ROC AUC: 0.9436\n",
      "Baseline_RF               - F1: 0.7105, ROC AUC: 0.8897\n",
      "Balanced_RF               - F1: 0.6903, ROC AUC: 0.8910\n",
      "\n",
      "BEST MODEL: Optimized_XGB\n",
      "F1 Score: 0.8773\n",
      "ROC AUC OvR: 0.9743\n",
      "Best model 'Optimized_XGB' is ready for use!\n"
     ]
    }
   ],
   "source": [
    "# load balanced data\n",
    "data = pd.read_csv('training_balanced_data.csv')\n",
    "X = data.drop(columns=['Outcome', 'Id'])\n",
    "y = data['Outcome']\n",
    "\n",
    "#split in training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Step 1: Define evaluation function\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
    "    \n",
    "    return f1, roc_auc\n",
    "\n",
    "# Step 2: Enhanced preprocessing pipeline - FIXED: Use regular sklearn pipeline for preprocessing\n",
    "def create_enhanced_preprocessing():\n",
    "    preprocessing = make_pipeline(\n",
    "        KNNImputer(n_neighbors=3),\n",
    "        StandardScaler()\n",
    "    )\n",
    "    return preprocessing\n",
    "\n",
    "# Step 3: Try different classifiers with optimized pipelines - FIXED: Use imblearn Pipeline directly\n",
    "def create_xgb_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('xgb', XGBClassifier(random_state=42, eval_metric='mlogloss'))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def create_svm_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=100)),\n",
    "        ('svm', SVC(probability=True, random_state=42))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def create_voting_pipeline():\n",
    "    # Individual classifiers\n",
    "    rf = RandomForestClassifier(n_estimators=250, random_state=42)\n",
    "    xgb_clf = XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "    lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    \n",
    "    # Voting classifier\n",
    "    voting = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', rf),\n",
    "            ('xgb', xgb_clf),\n",
    "            ('lr', lr)\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('voting', voting)\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def create_balanced_rf_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('balanced_rf', BalancedRandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# Step 4: Hyperparameter tuning for the best performing model\n",
    "def optimize_xgb_hyperparameters(X_train, y_train):\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('xgb', XGBClassifier(random_state=42, eval_metric='mlogloss'))\n",
    "    ])\n",
    "    \n",
    "    param_grid = {\n",
    "        'xgb__n_estimators': [100, 200, 300],\n",
    "        'xgb__max_depth': [3, 5, 7],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'xgb__subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grid, cv=5, scoring='f1_macro',  # Reduced cv from 10 to 5 for speed\n",
    "        n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_score_\n",
    "\n",
    "# Step 5: Try SMOTE for handling class imbalance\n",
    "def create_smote_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('xgb', XGBClassifier(random_state=42, eval_metric='mlogloss'))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# Step 6: Feature selection approach\n",
    "def create_feature_selection_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(f_classif, k=min(500, X_train.shape[1]))),  # Ensure k doesn't exceed features\n",
    "        ('xgb', XGBClassifier(random_state=42, eval_metric='mlogloss'))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# Step 7: Evaluate all models using cross-validation\n",
    "def evaluate_all_models(X_train, y_train):\n",
    "    models = {\n",
    "        'XGBoost': create_xgb_pipeline(),\n",
    "        'SVM': create_svm_pipeline(),\n",
    "        'Voting_Classifier': create_voting_pipeline(),\n",
    "        'Balanced_RF': create_balanced_rf_pipeline(),\n",
    "        'SMOTE_XGB': create_smote_pipeline(),\n",
    "        'Feature_Selection_XGB': create_feature_selection_pipeline()\n",
    "    }\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # Reduced from 10 to 5 for speed\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Cross-validation scores\n",
    "            cv_scores_f1 = cross_val_score(model, X_train, y_train, \n",
    "                                         cv=cv, scoring='f1_macro', n_jobs=-1)\n",
    "            cv_scores_roc = cross_val_score(model, X_train, y_train,\n",
    "                                          cv=cv, scoring='roc_auc_ovr', n_jobs=-1)\n",
    "            \n",
    "            results[name] = {\n",
    "                'f1_mean': cv_scores_f1.mean(),\n",
    "                'f1_std': cv_scores_f1.std(),\n",
    "                'roc_auc_mean': cv_scores_roc.mean(),\n",
    "                'roc_auc_std': cv_scores_roc.std()\n",
    "            }\n",
    "            \n",
    "            print(f\"{name} - F1: {cv_scores_f1.mean():.4f} (±{cv_scores_f1.std():.4f}), \"\n",
    "                  f\"ROC AUC: {cv_scores_roc.mean():.4f} (±{cv_scores_roc.std():.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {name}: {e}\")\n",
    "            results[name] = {\n",
    "                'f1_mean': 0,\n",
    "                'f1_std': 0,\n",
    "                'roc_auc_mean': 0,\n",
    "                'roc_auc_std': 0\n",
    "            }\n",
    "    \n",
    "    return results, models\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Evaluating all models...\")\n",
    "results, models = evaluate_all_models(X_train, y_train)\n",
    "\n",
    "# Step 8: Hyperparameter optimization for the best model\n",
    "print(\"\\nOptimizing hyperparameters for XGBoost...\")\n",
    "try:\n",
    "    best_xgb_model, best_score = optimize_xgb_hyperparameters(X_train, y_train)\n",
    "    print(f\"Best cross-validation score: {best_score:.4f}\")\n",
    "    # Add optimized model to our models dictionary\n",
    "    models['Optimized_XGB'] = best_xgb_model\n",
    "except Exception as e:\n",
    "    print(f\"Hyperparameter optimization failed: {e}\")\n",
    "    # Use default XGBoost as fallback\n",
    "    models['Optimized_XGB'] = create_xgb_pipeline()\n",
    "\n",
    "# Step 9: Train final models on full training data and evaluate on test set\n",
    "print(\"\\nTraining final models on full training data...\")\n",
    "final_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        f1, roc_auc = evaluate_model(model, X_test, y_test)\n",
    "        final_results[name] = {'f1': f1, 'roc_auc': roc_auc}\n",
    "        print(f\"{name} - Test F1: {f1:.4f}, Test ROC AUC: {roc_auc:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {name}: {e}\")\n",
    "        final_results[name] = {'f1': 0, 'roc_auc': 0}\n",
    "\n",
    "# Step 10: Compare with previous models from steps 3, 5, 7, 9\n",
    "# Create simple baseline models for comparison\n",
    "print(\"\\nEvaluating baseline models on test set...\")\n",
    "\n",
    "# Simple baseline models\n",
    "baseline_models = {\n",
    "    'Baseline_LR': make_pipeline(\n",
    "        SimpleImputer(strategy='mean'), \n",
    "        StandardScaler(), \n",
    "        LogisticRegression(max_iter=1000, random_state=42)\n",
    "    ),\n",
    "    'Baseline_RF': make_pipeline(\n",
    "        SimpleImputer(strategy='mean'),\n",
    "        StandardScaler(),\n",
    "        RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    ),\n",
    "    'Baseline_XGB': make_pipeline(\n",
    "        SimpleImputer(strategy='mean'),\n",
    "        StandardScaler(),\n",
    "        XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "    )\n",
    "}\n",
    "\n",
    "for name, model in baseline_models.items():\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        f1, roc_auc = evaluate_model(model, X_test, y_test)\n",
    "        final_results[name] = {'f1': f1, 'roc_auc': roc_auc}\n",
    "        print(f\"{name} - Test F1: {f1:.4f}, Test ROC AUC: {roc_auc:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {name}: {e}\")\n",
    "        final_results[name] = {'f1': 0, 'roc_auc': 0}\n",
    "\n",
    "# Step 11: Display final comparison\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL COMPARISON ON TEST SET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sort by F1 score\n",
    "sorted_results = sorted(final_results.items(), key=lambda x: x[1]['f1'], reverse=True)\n",
    "\n",
    "for name, scores in sorted_results:\n",
    "    print(f\"{name:25} - F1: {scores['f1']:.4f}, ROC AUC: {scores['roc_auc']:.4f}\")\n",
    "\n",
    "# Identify best model\n",
    "if sorted_results:\n",
    "    best_model_name, best_scores = sorted_results[0]\n",
    "    print(f\"\\nBEST MODEL: {best_model_name}\")\n",
    "    print(f\"F1 Score: {best_scores['f1']:.4f}\")\n",
    "    print(f\"ROC AUC OvR: {best_scores['roc_auc']:.4f}\")\n",
    "    \n",
    "    # Save the best model\n",
    "    best_model = None\n",
    "    if best_model_name in models:\n",
    "        best_model = models[best_model_name]\n",
    "    elif best_model_name in baseline_models:\n",
    "        best_model = baseline_models[best_model_name]\n",
    "    \n",
    "    if best_model is not None:\n",
    "        print(f\"Best model '{best_model_name}' is ready for use!\")\n",
    "else:\n",
    "    print(\"No models were successfully trained.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
