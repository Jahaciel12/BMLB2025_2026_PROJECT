{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a0994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional dependencies\n",
    "# Import dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "import keras\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load and prepare the data (assuming this part is already done)\n",
    "# X_train, X_test, y_train, y_test should be available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a59d8db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating all models...\n",
      "Evaluating XGBoost...\n",
      "XGBoost - F1: 0.7857 (±0.0126), ROC AUC: 0.9481 (±0.0088)\n",
      "Evaluating SVM...\n",
      "SVM - F1: 0.8571 (±0.0195), ROC AUC: 0.9736 (±0.0095)\n",
      "Evaluating Voting_Classifier...\n",
      "Voting_Classifier - F1: 0.8217 (±0.0214), ROC AUC: 0.9596 (±0.0085)\n",
      "Evaluating Balanced_RF...\n",
      "Balanced_RF - F1: 0.6807 (±0.0180), ROC AUC: 0.8704 (±0.0081)\n",
      "Evaluating SMOTE_XGB...\n",
      "SMOTE_XGB - F1: 0.7856 (±0.0266), ROC AUC: 0.9471 (±0.0101)\n",
      "Evaluating Feature_Selection_XGB...\n",
      "Feature_Selection_XGB - F1: 0.8070 (±0.0246), ROC AUC: 0.9552 (±0.0087)\n",
      "\n",
      "Optimizing hyperparameters for XGBoost...\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best cross-validation score: 0.8314\n",
      "\n",
      "Training final models on full training data...\n",
      "Training XGBoost...\n",
      "XGBoost - Test F1: 0.8275, Test ROC AUC: 0.9658\n",
      "Training SVM...\n",
      "SVM - Test F1: 0.8759, Test ROC AUC: 0.9773\n",
      "Training Voting_Classifier...\n",
      "Voting_Classifier - Test F1: 0.8753, Test ROC AUC: 0.9721\n",
      "Training Balanced_RF...\n",
      "Balanced_RF - Test F1: 0.6903, Test ROC AUC: 0.8910\n",
      "Training SMOTE_XGB...\n",
      "SMOTE_XGB - Test F1: 0.8272, Test ROC AUC: 0.9684\n",
      "Training Feature_Selection_XGB...\n",
      "Feature_Selection_XGB - Test F1: 0.8399, Test ROC AUC: 0.9658\n",
      "Training Optimized_XGB...\n",
      "Optimized_XGB - Test F1: 0.8773, Test ROC AUC: 0.9743\n",
      "\n",
      "Evaluating baseline models on test set...\n",
      "Baseline_LR - Test F1: 0.8043, Test ROC AUC: 0.9436\n",
      "Baseline_RF - Test F1: 0.7105, Test ROC AUC: 0.8897\n",
      "Baseline_XGB - Test F1: 0.8124, Test ROC AUC: 0.9614\n",
      "\n",
      "==================================================\n",
      "FINAL COMPARISON ON TEST SET\n",
      "==================================================\n",
      "Optimized_XGB             - F1: 0.8773, ROC AUC: 0.9743\n",
      "SVM                       - F1: 0.8759, ROC AUC: 0.9773\n",
      "Voting_Classifier         - F1: 0.8753, ROC AUC: 0.9721\n",
      "Feature_Selection_XGB     - F1: 0.8399, ROC AUC: 0.9658\n",
      "XGBoost                   - F1: 0.8275, ROC AUC: 0.9658\n",
      "SMOTE_XGB                 - F1: 0.8272, ROC AUC: 0.9684\n",
      "Baseline_XGB              - F1: 0.8124, ROC AUC: 0.9614\n",
      "Baseline_LR               - F1: 0.8043, ROC AUC: 0.9436\n",
      "Baseline_RF               - F1: 0.7105, ROC AUC: 0.8897\n",
      "Balanced_RF               - F1: 0.6903, ROC AUC: 0.8910\n",
      "\n",
      "BEST MODEL: Optimized_XGB\n",
      "F1 Score: 0.8773\n",
      "ROC AUC OvR: 0.9743\n",
      "Best model 'Optimized_XGB' is ready for use!\n"
     ]
    }
   ],
   "source": [
    "# load balanced data\n",
    "data = pd.read_csv('training_balanced_data.csv')\n",
    "X = data.drop(columns=['Outcome', 'Id'])\n",
    "y = data['Outcome']\n",
    "\n",
    "#split in training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Step 1: Define evaluation function\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
    "    \n",
    "    return f1, roc_auc\n",
    "\n",
    "# Step 2: Enhanced preprocessing pipeline - FIXED: Use regular sklearn pipeline for preprocessing\n",
    "def create_enhanced_preprocessing():\n",
    "    preprocessing = make_pipeline(\n",
    "        KNNImputer(n_neighbors=3),\n",
    "        StandardScaler()\n",
    "    )\n",
    "    return preprocessing\n",
    "\n",
    "# Step 3: Try different classifiers with optimized pipelines - FIXED: Use imblearn Pipeline directly\n",
    "def create_xgb_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('xgb', XGBClassifier(random_state=42, eval_metric='mlogloss'))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def create_svm_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=100)),\n",
    "        ('svm', SVC(probability=True, random_state=42))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def create_voting_pipeline():\n",
    "    # Individual classifiers\n",
    "    rf = RandomForestClassifier(n_estimators=250, random_state=42)\n",
    "    xgb_clf = XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "    lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    \n",
    "    # Voting classifier\n",
    "    voting = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', rf),\n",
    "            ('xgb', xgb_clf),\n",
    "            ('lr', lr)\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('voting', voting)\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def create_balanced_rf_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('balanced_rf', BalancedRandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# Step 4: Hyperparameter tuning for the best performing model\n",
    "def optimize_xgb_hyperparameters(X_train, y_train):\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('xgb', XGBClassifier(random_state=42, eval_metric='mlogloss'))\n",
    "    ])\n",
    "    \n",
    "    param_grid = {\n",
    "        'xgb__n_estimators': [100, 200, 300],\n",
    "        'xgb__max_depth': [3, 5, 7],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'xgb__subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grid, cv=5, scoring='f1_macro',  # Reduced cv from 10 to 5 for speed\n",
    "        n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_score_\n",
    "\n",
    "# Step 5: Try SMOTE for handling class imbalance\n",
    "def create_smote_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('xgb', XGBClassifier(random_state=42, eval_metric='mlogloss'))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# Step 6: Feature selection approach\n",
    "def create_feature_selection_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(f_classif, k=min(500, X_train.shape[1]))),  # Ensure k doesn't exceed features\n",
    "        ('xgb', XGBClassifier(random_state=42, eval_metric='mlogloss'))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# Step 7: Evaluate all models using cross-validation\n",
    "def evaluate_all_models(X_train, y_train):\n",
    "    models = {\n",
    "        'XGBoost': create_xgb_pipeline(),\n",
    "        'SVM': create_svm_pipeline(),\n",
    "        'Voting_Classifier': create_voting_pipeline(),\n",
    "        'Balanced_RF': create_balanced_rf_pipeline(),\n",
    "        'SMOTE_XGB': create_smote_pipeline(),\n",
    "        'Feature_Selection_XGB': create_feature_selection_pipeline()\n",
    "    }\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # Reduced from 10 to 5 for speed\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Cross-validation scores\n",
    "            cv_scores_f1 = cross_val_score(model, X_train, y_train, \n",
    "                                         cv=cv, scoring='f1_macro', n_jobs=-1)\n",
    "            cv_scores_roc = cross_val_score(model, X_train, y_train,\n",
    "                                          cv=cv, scoring='roc_auc_ovr', n_jobs=-1)\n",
    "            \n",
    "            results[name] = {\n",
    "                'f1_mean': cv_scores_f1.mean(),\n",
    "                'f1_std': cv_scores_f1.std(),\n",
    "                'roc_auc_mean': cv_scores_roc.mean(),\n",
    "                'roc_auc_std': cv_scores_roc.std()\n",
    "            }\n",
    "            \n",
    "            print(f\"{name} - F1: {cv_scores_f1.mean():.4f} (±{cv_scores_f1.std():.4f}), \"\n",
    "                  f\"ROC AUC: {cv_scores_roc.mean():.4f} (±{cv_scores_roc.std():.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {name}: {e}\")\n",
    "            results[name] = {\n",
    "                'f1_mean': 0,\n",
    "                'f1_std': 0,\n",
    "                'roc_auc_mean': 0,\n",
    "                'roc_auc_std': 0\n",
    "            }\n",
    "    \n",
    "    return results, models\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Evaluating all models...\")\n",
    "results, models = evaluate_all_models(X_train, y_train)\n",
    "\n",
    "# Step 8: Hyperparameter optimization for the best model\n",
    "print(\"\\nOptimizing hyperparameters for XGBoost...\")\n",
    "try:\n",
    "    best_xgb_model, best_score = optimize_xgb_hyperparameters(X_train, y_train)\n",
    "    print(f\"Best cross-validation score: {best_score:.4f}\")\n",
    "    # Add optimized model to our models dictionary\n",
    "    models['Optimized_XGB'] = best_xgb_model\n",
    "except Exception as e:\n",
    "    print(f\"Hyperparameter optimization failed: {e}\")\n",
    "    # Use default XGBoost as fallback\n",
    "    models['Optimized_XGB'] = create_xgb_pipeline()\n",
    "\n",
    "# Step 9: Train final models on full training data and evaluate on test set\n",
    "print(\"\\nTraining final models on full training data...\")\n",
    "final_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        f1, roc_auc = evaluate_model(model, X_test, y_test)\n",
    "        final_results[name] = {'f1': f1, 'roc_auc': roc_auc}\n",
    "        print(f\"{name} - Test F1: {f1:.4f}, Test ROC AUC: {roc_auc:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {name}: {e}\")\n",
    "        final_results[name] = {'f1': 0, 'roc_auc': 0}\n",
    "\n",
    "# Step 10: Compare with previous models from steps 3, 5, 7, 9\n",
    "# Create simple baseline models for comparison\n",
    "print(\"\\nEvaluating baseline models on test set...\")\n",
    "\n",
    "# Simple baseline models\n",
    "baseline_models = {\n",
    "    'Baseline_LR': make_pipeline(\n",
    "        SimpleImputer(strategy='mean'), \n",
    "        StandardScaler(), \n",
    "        LogisticRegression(max_iter=1000, random_state=42)\n",
    "    ),\n",
    "    'Baseline_RF': make_pipeline(\n",
    "        SimpleImputer(strategy='mean'),\n",
    "        StandardScaler(),\n",
    "        RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    ),\n",
    "    'Baseline_XGB': make_pipeline(\n",
    "        SimpleImputer(strategy='mean'),\n",
    "        StandardScaler(),\n",
    "        XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "    )\n",
    "}\n",
    "\n",
    "for name, model in baseline_models.items():\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        f1, roc_auc = evaluate_model(model, X_test, y_test)\n",
    "        final_results[name] = {'f1': f1, 'roc_auc': roc_auc}\n",
    "        print(f\"{name} - Test F1: {f1:.4f}, Test ROC AUC: {roc_auc:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {name}: {e}\")\n",
    "        final_results[name] = {'f1': 0, 'roc_auc': 0}\n",
    "\n",
    "# Step 11: Display final comparison\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL COMPARISON ON TEST SET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sort by F1 score\n",
    "sorted_results = sorted(final_results.items(), key=lambda x: x[1]['f1'], reverse=True)\n",
    "\n",
    "for name, scores in sorted_results:\n",
    "    print(f\"{name:25} - F1: {scores['f1']:.4f}, ROC AUC: {scores['roc_auc']:.4f}\")\n",
    "\n",
    "# Identify best model\n",
    "if sorted_results:\n",
    "    best_model_name, best_scores = sorted_results[0]\n",
    "    print(f\"\\nBEST MODEL: {best_model_name}\")\n",
    "    print(f\"F1 Score: {best_scores['f1']:.4f}\")\n",
    "    print(f\"ROC AUC OvR: {best_scores['roc_auc']:.4f}\")\n",
    "    \n",
    "    # Save the best model\n",
    "    best_model = None\n",
    "    if best_model_name in models:\n",
    "        best_model = models[best_model_name]\n",
    "    elif best_model_name in baseline_models:\n",
    "        best_model = baseline_models[best_model_name]\n",
    "    \n",
    "    if best_model is not None:\n",
    "        print(f\"Best model '{best_model_name}' is ready for use!\")\n",
    "else:\n",
    "    print(\"No models were successfully trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f41ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating all models...\n",
      "Evaluating XGBoost...\n",
      "XGBoost - F1: 0.7857 (±0.0126), ROC AUC: 0.9481 (±0.0088)\n",
      "Evaluating SVM...\n",
      "SVM - F1: 0.8570 (±0.0200), ROC AUC: 0.9741 (±0.0086)\n",
      "\n",
      "Optimizing hyperparameters for XGBoost...\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best cross-validation score: 0.8314\n",
      "\n",
      "Training final models on full training data...\n",
      "Training XGBoost...\n",
      "XGBoost - Test F1: 0.8275, Test ROC AUC: 0.9658\n",
      "Training SVM...\n",
      "SVM - Test F1: 0.8813, Test ROC AUC: 0.9778\n",
      "Training Optimized_XGB...\n",
      "Optimized_XGB - Test F1: 0.8773, Test ROC AUC: 0.9743\n",
      "\n",
      "Evaluating baseline models on test set...\n",
      "these are not executed\n",
      "\n",
      "==================================================\n",
      "FINAL COMPARISON ON TEST SET\n",
      "==================================================\n",
      "SVM                       - F1: 0.8813, ROC AUC: 0.9778\n",
      "Optimized_XGB             - F1: 0.8773, ROC AUC: 0.9743\n",
      "XGBoost                   - F1: 0.8275, ROC AUC: 0.9658\n",
      "\n",
      "BEST MODEL: SVM\n",
      "F1 Score: 0.8813\n",
      "ROC AUC OvR: 0.9778\n",
      "Best model 'SVM' is ready for use!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load data\n",
    "\n",
    "data = pd.read_csv('training_balanced_data.csv')\n",
    "X = data.drop(columns=['Outcome', 'Id'])\n",
    "y = data['Outcome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
    "    \n",
    "    return f1, roc_auc\n",
    "\n",
    "\n",
    "# Pipelines\n",
    "\n",
    "def create_xgb_pipeline():\n",
    "    return Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('xgb', XGBClassifier(random_state=42, eval_metric='mlogloss'))\n",
    "    ])\n",
    "\n",
    "def create_svm_pipeline():\n",
    "    return Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=100)),\n",
    "        ('svm', SVC(probability=True, random_state=42))\n",
    "    ])\n",
    "\n",
    "\n",
    "# Hyperparameter optimisation (XGB)\n",
    "\n",
    "def optimize_xgb_hyperparameters(X_train, y_train):\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('xgb', XGBClassifier(random_state=42, eval_metric='mlogloss'))\n",
    "    ])\n",
    "    \n",
    "    param_grid = {\n",
    "        'xgb__n_estimators': [100, 200, 300], # number of trees in the ensemble\n",
    "        'xgb__max_depth': [3, 5, 7], # max depth of each tree\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.2], # Step size shrinkage used to prevent overfitting\n",
    "        'xgb__subsample': [0.8, 0.9, 1.0] # reduces overfitting and introduces randomness like bagging\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grid, cv=5, scoring='f1_macro',\n",
    "        n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_, grid_search.best_score_\n",
    "\n",
    "\n",
    "# Models \n",
    "\n",
    "def evaluate_all_models(X_train, y_train):\n",
    "\n",
    "    models = {\n",
    "        'XGBoost': create_xgb_pipeline(),\n",
    "        'SVM': create_svm_pipeline()\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        try:\n",
    "            cv_scores_f1 = cross_val_score(\n",
    "                model, X_train, y_train, cv=cv,\n",
    "                scoring='f1_macro', n_jobs=-1\n",
    "            )\n",
    "            cv_scores_roc = cross_val_score(\n",
    "                model, X_train, y_train, cv=cv,\n",
    "                scoring='roc_auc_ovr', n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            results[name] = {\n",
    "                'f1_mean': cv_scores_f1.mean(),\n",
    "                'f1_std': cv_scores_f1.std(),\n",
    "                'roc_auc_mean': cv_scores_roc.mean(),\n",
    "                'roc_auc_std': cv_scores_roc.std()\n",
    "            }\n",
    "            \n",
    "            print(f\"{name} - F1: {cv_scores_f1.mean():.4f} (±{cv_scores_f1.std():.4f}), \"\n",
    "                  f\"ROC AUC: {cv_scores_roc.mean():.4f} (±{cv_scores_roc.std():.4f})\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {name}: {e}\")\n",
    "            results[name] = {\n",
    "                'f1_mean': 0, 'f1_std': 0,\n",
    "                'roc_auc_mean': 0, 'roc_auc_std': 0\n",
    "            }\n",
    "    \n",
    "    return results, models\n",
    "\n",
    "\n",
    "\n",
    "# Run models \n",
    "\n",
    "print(\"Evaluating all models...\")\n",
    "results, models = evaluate_all_models(X_train, y_train)\n",
    "\n",
    "\n",
    "# Hyperparam optimisation for XGB\n",
    "\n",
    "print(\"\\nOptimizing hyperparameters for XGBoost...\")\n",
    "\n",
    "try:\n",
    "    best_xgb_model, best_score = optimize_xgb_hyperparameters(X_train, y_train)\n",
    "    print(f\"Best cross-validation score: {best_score:.4f}\")\n",
    "    models['Optimized_XGB'] = best_xgb_model\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Hyperparameter optimization failed: {e}\")\n",
    "    models['Optimized_XGB'] = create_xgb_pipeline()\n",
    "\n",
    "\n",
    "# Final training and test\n",
    "\n",
    "print(\"\\nTraining final models on full training data...\")\n",
    "final_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        f1, roc_auc = evaluate_model(model, X_test, y_test)\n",
    "        \n",
    "        final_results[name] = {'f1': f1, 'roc_auc': roc_auc}\n",
    "        \n",
    "        print(f\"{name} - Test F1: {f1:.4f}, Test ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {name}: {e}\")\n",
    "        final_results[name] = {'f1': 0, 'roc_auc': 0}\n",
    "\n",
    "\n",
    "# Comparison\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Final comparision on test set\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "sorted_results = sorted(final_results.items(), key=lambda x: x[1]['f1'], reverse=True)\n",
    "\n",
    "for name, scores in sorted_results:\n",
    "    print(f\"{name:25} - F1: {scores['f1']:.4f}, ROC AUC: {scores['roc_auc']:.4f}\")\n",
    "\n",
    "if sorted_results:\n",
    "    best_model_name, best_scores = sorted_results[0]\n",
    "    print(f\"\\nBEST MODEL: {best_model_name}\")\n",
    "    print(f\"F1 Score: {best_scores['f1']:.4f}\")\n",
    "    print(f\"ROC AUC OvR: {best_scores['roc_auc']:.4f}\")\n",
    "\n",
    "    if best_model_name in models:\n",
    "        best_model = models[best_model_name]\n",
    "    else:\n",
    "        best_model = baseline_models[best_model_name]\n",
    "\n",
    "    print(f\"Best model '{best_model_name}' is ready for use!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
