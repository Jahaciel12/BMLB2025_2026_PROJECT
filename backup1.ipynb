{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef00a265-8616-4a61-b556-37fce641d5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages?\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d28017-d84f-4b2a-97db-38d374619431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 data exploratory\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load in dataset\n",
    "train_data = pd.read_csv('train_2025_2026.csv')\n",
    "print(train_data.head())\n",
    "\n",
    "# Missing values summary\n",
    "missing_by_column = train_data.isnull().sum()\n",
    "columns_with_missing = missing_by_column[missing_by_column > 0]\n",
    "print(f\"\\nColumns with missing values: {len(columns_with_missing)}\")\n",
    "print(f\"Columns without missing values: {len(train_data.columns) - len(columns_with_missing)}\")\n",
    "\n",
    "# Row-level missing info\n",
    "rows_with_any_missing = train_data.isnull().any(axis=1).sum()\n",
    "rows_with_all_missing = train_data.isnull().all(axis=1).sum()\n",
    "rows_with_half_missing = (train_data.isnull().sum(axis=1) > train_data.shape[1]//2).sum()\n",
    "\n",
    "print(f\"Rows with any missing values: {rows_with_any_missing} ({rows_with_any_missing/len(train_data)*100:.1f}%)\")\n",
    "print(f\"Rows with all values missing: {rows_with_all_missing} ({rows_with_all_missing/len(train_data)*100:.1f}%)\")\n",
    "print(f\"Rows with >50% values missing: {rows_with_half_missing} ({rows_with_half_missing/len(train_data)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nMissing value patterns (first 10 columns with most missing):\")\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'Missing_Count': train_data.isnull().sum(),\n",
    "    'Missing_Percentage': (train_data.isnull().sum() / len(train_data)) * 100\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "print(missing_analysis.head(10))\n",
    "\n",
    "# Class proportions\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Proportion of classes in data\")\n",
    "class_counts = train_data['Outcome'].value_counts()\n",
    "class_proportions = train_data['Outcome'].value_counts(normalize=True) * 100\n",
    "print(\"Absolute counts:\")\n",
    "print(class_counts)\n",
    "print(\"\\nProportions (%):\")\n",
    "print(class_proportions)\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = train_data.corr()\n",
    "print(corr_matrix)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPLORATORY DATA ANALYSIS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset: {train_data.shape[0]} rows, {train_data.shape[1]} columns\")\n",
    "print(f\"Total missing values: {train_data.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7886cfe9-08ba-467f-9185-773fa3f238c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guided Exercise 2: Undersampling to balance classes\n",
    "from sklearn.utils import resample\n",
    "imbalanced_data = pd.read_csv('train_2025_2026.csv')\n",
    "\n",
    "class_counts = imbalanced_data['Outcome'].value_counts()\n",
    "print(class_counts)\n",
    "num_min = min(class_counts)\n",
    "print('Number of samples of the class with fewer samples:', num_min)\n",
    "\n",
    "balanced_list = []\n",
    "for class_ in imbalanced_data['Outcome'].unique():\n",
    "    df_class = imbalanced_data[imbalanced_data['Outcome'] == class_]\n",
    "    df_resampled = resample(df_class, replace=False, n_samples=num_min, random_state=42)\n",
    "    balanced_list.append(df_resampled)\n",
    "\n",
    "balanced_data = pd.concat(balanced_list)\n",
    "print('New class distribution:\\n', balanced_data['Outcome'].value_counts())\n",
    "balanced_data.to_csv('training_balanced_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69032a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression model pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('training_balanced_data.csv')\n",
    "X = data.drop(columns=['Outcome', 'Id'])\n",
    "y = data['Outcome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = make_pipeline(SimpleImputer(), StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "model.fit(X_train, y_train)\n",
    "score = model.score(X_test, y_test)\n",
    "print(f\"Model Accuracy: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BMLB2025_2026",
   "language": "python",
   "name": "bmlb2025_2026"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
