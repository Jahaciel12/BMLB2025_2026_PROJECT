{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef00a265-8616-4a61-b556-37fce641d5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages?\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21d28017-d84f-4b2a-97db-38d374619431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXPLORATORY DATA ANALYSIS\n",
      "==================================================\n",
      "   Gene 1  Gene 2  Gene 3   Gene 4  Gene 5  Gene 6  Gene 7  Gene 8  Gene 9  \\\n",
      "0     NaN   133.0    75.0  12971.0   401.0   296.0   348.0  1087.0  4224.0   \n",
      "1     5.0   127.0    85.0  15875.0   285.0   305.0   298.0  1297.0  2090.0   \n",
      "2     5.0    81.0   133.0      NaN   228.0   387.0   465.0  2657.0  3653.0   \n",
      "3     5.0    75.0     NaN  17527.0   467.0   242.0   392.0  3482.0  3408.0   \n",
      "4     4.0    74.0   103.0  13053.0   329.0     NaN   357.0  2714.0  2739.0   \n",
      "\n",
      "   Gene 10  ...  Gene 1993  Gene 1994  Gene 1995  Gene 1996  Gene 1997  \\\n",
      "0      NaN  ...      101.0    16517.0      308.0      100.0      235.0   \n",
      "1      8.0  ...       64.0    17950.0      458.0      191.0      137.0   \n",
      "2     32.0  ...      138.0    11715.0      181.0      161.0      261.0   \n",
      "3     59.0  ...       88.0    12420.0      403.0      114.0      290.0   \n",
      "4     75.0  ...      129.0    11697.0      205.0      102.0      269.0   \n",
      "\n",
      "   Gene 1998  Gene 1999  Gene 2000  Outcome    Id  \n",
      "0       55.0      219.0      348.0        3  2654  \n",
      "1      110.0      204.0      346.0        0  1477  \n",
      "2       60.0      150.0      336.0        2  1794  \n",
      "3      110.0      244.0      204.0        2  2058  \n",
      "4       66.0      187.0       53.0        0  1375  \n",
      "\n",
      "[5 rows x 2002 columns]\n",
      "\n",
      "Columns with missing values: 2000\n",
      "Columns without missing values: 2\n",
      "Rows with any missing values: 3520 (100.0%)\n",
      "Rows with all values missing: 0 (0.0%)\n",
      "Rows with >50% values missing: 200 (5.7%)\n",
      "\n",
      "Missing value patterns (first 10 columns with most missing):\n",
      "           Missing_Count  Missing_Percentage\n",
      "Gene 1511            290            8.238636\n",
      "Gene 1797            289            8.210227\n",
      "Gene 205             288            8.181818\n",
      "Gene 775             286            8.125000\n",
      "Gene 249             284            8.068182\n",
      "Gene 1474            283            8.039773\n",
      "Gene 984             283            8.039773\n",
      "Gene 11              283            8.039773\n",
      "Gene 1439            282            8.011364\n",
      "Gene 1606            282            8.011364\n",
      "\n",
      "==================================================\n",
      "Proportion of classes in data\n",
      "Absolute counts:\n",
      "Outcome\n",
      "3    1280\n",
      "2     881\n",
      "1     730\n",
      "0     629\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Proportions (%):\n",
      "Outcome\n",
      "3    36.363636\n",
      "2    25.028409\n",
      "1    20.738636\n",
      "0    17.869318\n",
      "Name: proportion, dtype: float64\n",
      "             Gene 1    Gene 2    Gene 3    Gene 4    Gene 5    Gene 6  \\\n",
      "Gene 1     1.000000  0.029594 -0.002196  0.040565  0.019933 -0.022916   \n",
      "Gene 2     0.029594  1.000000  0.014415  0.003283  0.009746 -0.005038   \n",
      "Gene 3    -0.002196  0.014415  1.000000  0.006650 -0.019600  0.013939   \n",
      "Gene 4     0.040565  0.003283  0.006650  1.000000 -0.005472 -0.016048   \n",
      "Gene 5     0.019933  0.009746 -0.019600 -0.005472  1.000000 -0.036408   \n",
      "...             ...       ...       ...       ...       ...       ...   \n",
      "Gene 1998  0.005428  0.015530 -0.022892 -0.031576 -0.005858  0.015300   \n",
      "Gene 1999  0.001886 -0.043010  0.008495  0.007917  0.017335 -0.010509   \n",
      "Gene 2000  0.021234  0.032594  0.011763 -0.024210  0.001205  0.009290   \n",
      "Outcome   -0.008938 -0.008995 -0.011412  0.033195 -0.016812  0.015594   \n",
      "Id        -0.012059 -0.015073  0.003389 -0.007109  0.000172  0.011596   \n",
      "\n",
      "             Gene 7    Gene 8    Gene 9   Gene 10  ...  Gene 1993  Gene 1994  \\\n",
      "Gene 1     0.002952 -0.003056  0.008376 -0.007550  ...   0.008587  -0.005571   \n",
      "Gene 2     0.015817  0.022957 -0.020480 -0.008903  ...   0.012815   0.048474   \n",
      "Gene 3    -0.017886 -0.015247  0.033222  0.003308  ...   0.001968   0.000748   \n",
      "Gene 4     0.040413 -0.045491  0.081255  0.014528  ...  -0.005806   0.001049   \n",
      "Gene 5     0.012386  0.004408  0.025583 -0.006708  ...   0.012788   0.016881   \n",
      "...             ...       ...       ...       ...  ...        ...        ...   \n",
      "Gene 1998  0.005142 -0.032034 -0.012101 -0.032959  ...  -0.030742   0.034099   \n",
      "Gene 1999 -0.009280  0.013085 -0.028970 -0.001788  ...   0.000173  -0.004894   \n",
      "Gene 2000  0.014847 -0.024464 -0.019993 -0.010481  ...   0.017773  -0.002738   \n",
      "Outcome   -0.044739  0.083488  0.016540  0.002482  ...  -0.021696  -0.024077   \n",
      "Id         0.025611 -0.010310 -0.020079  0.028325  ...  -0.015265   0.013727   \n",
      "\n",
      "           Gene 1995  Gene 1996  Gene 1997  Gene 1998  Gene 1999  Gene 2000  \\\n",
      "Gene 1     -0.029273   0.028229   0.012327   0.005428   0.001886   0.021234   \n",
      "Gene 2     -0.000023  -0.001678  -0.011208   0.015530  -0.043010   0.032594   \n",
      "Gene 3      0.021922   0.011049  -0.000887  -0.022892   0.008495   0.011763   \n",
      "Gene 4     -0.009827  -0.011619  -0.041042  -0.031576   0.007917  -0.024210   \n",
      "Gene 5      0.011887   0.002865   0.011485  -0.005858   0.017335   0.001205   \n",
      "...              ...        ...        ...        ...        ...        ...   \n",
      "Gene 1998   0.016178  -0.008277  -0.024608   1.000000   0.001867  -0.014835   \n",
      "Gene 1999   0.013176  -0.004492   0.017794   0.001867   1.000000  -0.011333   \n",
      "Gene 2000  -0.003128  -0.015602  -0.015214  -0.014835  -0.011333   1.000000   \n",
      "Outcome     0.017662   0.001302  -0.025667   0.001999   0.013371  -0.003913   \n",
      "Id         -0.003320  -0.022955   0.003493   0.019356  -0.005503  -0.013286   \n",
      "\n",
      "            Outcome        Id  \n",
      "Gene 1    -0.008938 -0.012059  \n",
      "Gene 2    -0.008995 -0.015073  \n",
      "Gene 3    -0.011412  0.003389  \n",
      "Gene 4     0.033195 -0.007109  \n",
      "Gene 5    -0.016812  0.000172  \n",
      "...             ...       ...  \n",
      "Gene 1998  0.001999  0.019356  \n",
      "Gene 1999  0.013371 -0.005503  \n",
      "Gene 2000 -0.003913 -0.013286  \n",
      "Outcome    1.000000 -0.016504  \n",
      "Id        -0.016504  1.000000  \n",
      "\n",
      "[2002 rows x 2002 columns]\n",
      "\n",
      "==================================================\n",
      "EXPLORATORY DATA ANALYSIS SUMMARY\n",
      "==================================================\n",
      "Dataset: 3520 rows, 2002 columns\n",
      "Total missing values: 497848\n"
     ]
    }
   ],
   "source": [
    "# 1 data exploratory\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load in dataset\n",
    "train_data = pd.read_csv('train_2025_2026.csv')\n",
    "print(train_data.head())\n",
    "\n",
    "# Missing values summary\n",
    "missing_by_column = train_data.isnull().sum()\n",
    "columns_with_missing = missing_by_column[missing_by_column > 0]\n",
    "print(f\"\\nColumns with missing values: {len(columns_with_missing)}\")\n",
    "print(f\"Columns without missing values: {len(train_data.columns) - len(columns_with_missing)}\")\n",
    "\n",
    "# Row-level missing info\n",
    "rows_with_any_missing = train_data.isnull().any(axis=1).sum()\n",
    "rows_with_all_missing = train_data.isnull().all(axis=1).sum()\n",
    "rows_with_half_missing = (train_data.isnull().sum(axis=1) > train_data.shape[1]//2).sum()\n",
    "\n",
    "print(f\"Rows with any missing values: {rows_with_any_missing} ({rows_with_any_missing/len(train_data)*100:.1f}%)\")\n",
    "print(f\"Rows with all values missing: {rows_with_all_missing} ({rows_with_all_missing/len(train_data)*100:.1f}%)\")\n",
    "print(f\"Rows with >50% values missing: {rows_with_half_missing} ({rows_with_half_missing/len(train_data)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nMissing value patterns (first 10 columns with most missing):\")\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'Missing_Count': train_data.isnull().sum(),\n",
    "    'Missing_Percentage': (train_data.isnull().sum() / len(train_data)) * 100\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "print(missing_analysis.head(10))\n",
    "\n",
    "# Class proportions\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Proportion of classes in data\")\n",
    "class_counts = train_data['Outcome'].value_counts()\n",
    "class_proportions = train_data['Outcome'].value_counts(normalize=True) * 100\n",
    "print(\"Absolute counts:\")\n",
    "print(class_counts)\n",
    "print(\"\\nProportions (%):\")\n",
    "print(class_proportions)\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = train_data.corr()\n",
    "print(corr_matrix)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPLORATORY DATA ANALYSIS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset: {train_data.shape[0]} rows, {train_data.shape[1]} columns\")\n",
    "print(f\"Total missing values: {train_data.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7886cfe9-08ba-467f-9185-773fa3f238c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome\n",
      "3    1280\n",
      "2     881\n",
      "1     730\n",
      "0     629\n",
      "Name: count, dtype: int64\n",
      "Number of samples of the class with fewer samples: 629\n",
      "New class distribution:\n",
      " Outcome\n",
      "3    629\n",
      "0    629\n",
      "2    629\n",
      "1    629\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Guided Exercise 2: Undersampling to balance classes\n",
    "from sklearn.utils import resample\n",
    "imbalanced_data = pd.read_csv('train_2025_2026.csv')\n",
    "\n",
    "class_counts = imbalanced_data['Outcome'].value_counts()\n",
    "print(class_counts)\n",
    "num_min = min(class_counts)\n",
    "print('Number of samples of the class with fewer samples:', num_min)\n",
    "\n",
    "balanced_list = []\n",
    "for class_ in imbalanced_data['Outcome'].unique():\n",
    "    df_class = imbalanced_data[imbalanced_data['Outcome'] == class_]\n",
    "    df_resampled = resample(df_class, replace=False, n_samples=num_min, random_state=42)\n",
    "    balanced_list.append(df_resampled)\n",
    "\n",
    "balanced_data = pd.concat(balanced_list)\n",
    "print('New class distribution:\\n', balanced_data['Outcome'].value_counts())\n",
    "balanced_data.to_csv('training_balanced_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d69032a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ROC AUC: 1.0000\n",
      "Validation ROC AUC: 0.9408\n",
      "Training Macro F1: 1.0000\n",
      "Validation Macro F1: 0.7940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nThe impute and scaling of the data can't be done before the spliting of the training and validation data because then we would be imputing and\\nscaling taking into acount the average of all data including the one that will be used for testing, which will actually give information to the model\\nof the validation data and thus the generalisation poerformance accuracy will be compromised.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guided exercise 3:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('training_balanced_data.csv')\n",
    "X = data.drop(columns=['Outcome', 'Id'])\n",
    "y = data['Outcome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = make_pipeline(SimpleImputer(), StandardScaler(), LogisticRegression(max_iter=1000, penalty = None))\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "y_train_prob = model.predict_proba(X_train)\n",
    "y_test_prob = model.predict_proba(X_test)\n",
    "\n",
    "roc_train = roc_auc_score(y_train, y_train_prob, multi_class='ovr')\n",
    "roc_val = roc_auc_score(y_test, y_test_prob, multi_class='ovr')\n",
    "\n",
    "f1_train = f1_score(y_train, y_train_pred, average='macro')\n",
    "f1_val = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "print(f\"Training ROC AUC: {roc_train:.4f}\")\n",
    "print(f\"Validation ROC AUC: {roc_val:.4f}\")\n",
    "print(f\"Training Macro F1: {f1_train:.4f}\")\n",
    "print(f\"Validation Macro F1: {f1_val:.4f}\")\n",
    "\n",
    "'''\n",
    "Explanation of the metrics:\n",
    "\n",
    "ROC AUC one-versus-rest basically performs a normal ROC AUC for binary classification, which basically compares the performance\n",
    "of the model regarding true positive rate and false positive rate over all posible thresholds. But in this case, as we are in a\n",
    "multiclasification task, this AUC will be the average of comparing each class against all other classes togheter as if they \n",
    "formed a new class all toghether.\n",
    "\n",
    "Macro F1 is like a plain F1 in binary classification, where precission and recall are taken into acount and the average is done in a one-versus-rest \n",
    "as with the ROC AUC.\n",
    "'''\n",
    "'''\n",
    "The impute and scaling of the data can't be done before the spliting of the training and validation data because then we would be imputing and\n",
    "scaling taking into acount the average of all data including the one that will be used for testing, which will actually give information to the model\n",
    "of the validation data and thus the generalisation poerformance accuracy will be compromised.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f58cf55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ROC AUC: 1.0000\n",
      "Validation ROC AUC: 0.9478\n",
      "Training Macro F1: 1.0000\n",
      "Validation Macro F1: 0.8139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nIn this case, we first need to scale the data as the imputer algorithm uses euclidian distances to find the nearest neighbor in the data space,\\ntaking into acount all the dimensions, and so it's necessary to scale the data so not one feature has more weight than another when \\ncomputing which is the nearest neighbour.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guided exercise 4:\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "model = make_pipeline(StandardScaler(), KNNImputer(n_neighbors=3), LogisticRegression(penalty=None))\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "y_train_prob = model.predict_proba(X_train)\n",
    "y_test_prob = model.predict_proba(X_test)\n",
    "\n",
    "roc_train = roc_auc_score(y_train, y_train_prob, multi_class='ovr')\n",
    "roc_val = roc_auc_score(y_test, y_test_prob, multi_class='ovr')\n",
    "\n",
    "f1_train = f1_score(y_train, y_train_pred, average='macro')\n",
    "f1_val = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "print(f\"Training ROC AUC: {roc_train:.4f}\")\n",
    "print(f\"Validation ROC AUC: {roc_val:.4f}\")\n",
    "print(f\"Training Macro F1: {f1_train:.4f}\")\n",
    "print(f\"Validation Macro F1: {f1_val:.4f}\")\n",
    "\n",
    "\n",
    "'''\n",
    "In this case, we first need to scale the data as the imputer algorithm uses euclidian distances to find the nearest neighbor in the data space,\n",
    "taking into acount all the dimensions, and so it's necessary to scale the data so not one feature has more weight than another when \n",
    "computing which is the nearest neighbour.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12433cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 10-fold ROC AUC (OvR): 0.9427\n",
      "Average 10-fold F1 macro: 0.7856\n"
     ]
    }
   ],
   "source": [
    "# Guided exercise 5:\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "data = pd.read_csv('training_balanced_data.csv')\n",
    "X = data.drop(columns=['Outcome', 'Id'])\n",
    "y = data['Outcome']\n",
    "\n",
    "model = make_pipeline(StandardScaler(), KNNImputer(n_neighbors=3), LogisticRegression(C = 1))\n",
    "\n",
    "scoring = {\n",
    "    'roc_auc_ovr': make_scorer(roc_auc_score, multi_class='ovr', response_method='predict_proba'),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro')\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_results = cross_validate(model, X, y, cv=cv, scoring=scoring)\n",
    "\n",
    "roc_auc = np.mean(cv_results['test_roc_auc_ovr'])\n",
    "f1_macro = np.mean(cv_results['test_f1_macro'])\n",
    "\n",
    "print(f\"Average 10-fold ROC AUC (OvR): {roc_auc:.4f}\")\n",
    "print(f\"Average 10-fold F1 macro: {f1_macro:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BMLB2025_2026",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
